{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVumKqfcfvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ed8a03ab-d8ae-4310-8428-2d6cf551c7c5"
      },
      "source": [
        "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-25 10:06:32--  http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz [following]\n",
            "--2020-06-25 10:06:32--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3663376519 (3.4G) [application/gzip]\n",
            "Saving to: ‘download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz’\n",
            "\n",
            "download.php?f=Open 100%[===================>]   3.41G  22.9MB/s    in 2m 35s  \n",
            "\n",
            "2020-06-25 10:09:09 (22.5 MB/s) - ‘download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz’ saved [3663376519/3663376519]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHVWRYknd3rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip opensubs.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD2hgiUOk_73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "52f7c33f-20d6-44c6-9d80-e6dc38ff5225"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-25 10:14:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-06-25 10:14:19--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-06-25 10:14:19--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.00MB/s    in 6m 28s  \n",
            "\n",
            "2020-06-25 10:20:47 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rpSq85Cm8hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8c5c4f5-cf4a-4d74-d674-6a0d69664237"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8n6wwtqhfJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPSEIVmnnVhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeds = {}\n",
        "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeds[word] = vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKY8O4cBnlTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "693da106-e274-47ba-a19c-f36a8b12d513"
      },
      "source": [
        "print(embeds['nvidia'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.33102  -0.75616   1.9245    2.4399   -0.089388 -0.032776  0.51466\n",
            " -0.50618  -1.4438    0.91152   0.33239   0.36876  -0.18953  -0.76038\n",
            " -0.532    -0.68048  -1.7754    1.3991    0.46812  -1.3685    0.68417\n",
            " -1.4185   -0.38349  -0.66051   0.090225  0.34747  -0.39275  -0.65303\n",
            " -0.10736  -1.195    -0.050039 -0.56808  -0.064523 -0.45444   0.9146\n",
            "  0.19898  -0.38     -0.018667 -0.81614  -0.7589    0.73566  -0.35776\n",
            " -1.1478   -0.63147   0.18396  -0.7315    0.79278   0.46402   0.32069\n",
            " -0.050436]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGQEYdjGntlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = []\n",
        "i = 0\n",
        "with open(\"opensubs\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.lower()\n",
        "        dataset.append(values)\n",
        "        i = i + 1\n",
        "        if i == 1000000:\n",
        "          break"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akl_Kn_TpR9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 20000\n",
        "embedding_dim = 200\n",
        "max_length = 20               #max(len(i.split())  for i in dataset)\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(dataset)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(dataset)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type, padding = 'post')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1_npBPXtHCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a408e821-43e9-4daf-9cc9-bdd203daaf62"
      },
      "source": [
        "i = 5\n",
        "print(len(word_index))\n",
        "print(dataset[i])\n",
        "print(padded[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45068\n",
            "exactly two years ago today, she and i buried a time capsule here.\n",
            "\n",
            "[ 515  136  234  403  294   69    8    4 1594    6   82 6233   36    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPXqdTxhvHCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_word = {}\n",
        "for word , index in word_index.items():\n",
        "    index_word[index] = word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Ohf5c8ooT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7af87898-36c7-4846-b90f-53f13f85d874"
      },
      "source": [
        "input_seq = []\n",
        "input_labs = []\n",
        "j = 0\n",
        "for i in padded:\n",
        "  if j%2 == 0:\n",
        "    input_seq.append(i)\n",
        "  if j%2 == 1:\n",
        "    input_labs.append(i)\n",
        "  j+=1\n",
        "  if j == 20000:\n",
        "    break\n",
        "\n",
        "input_seq = np.array(input_seq)\n",
        "input_labs = np.array(input_labs)\n",
        "\n",
        "print(input_seq[3])\n",
        "print(input_labs[3])\n",
        "print(input_seq.shape)\n",
        "print(input_labs.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  27 1024    5  347   36  136  234  350   33   69  839   46  279    0\n",
            "    0    0    0    0    0    0]\n",
            "[ 30  87   5 169   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "(10000, 20)\n",
            "(10000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20-quv0ztTsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_seq = []\n",
        "\n",
        "# for i in range(500):\n",
        "#   sent_embeds = []\n",
        "#   temp1 = dataset[2*i].split()\n",
        "#   for j in temp1:\n",
        "#     if j in embeds:\n",
        "#       temp2 = embeds[j]\n",
        "#     else:\n",
        "#       temp2 = np.zeros((embedding_dim), dtype=np.float32)\n",
        "#     sent_embeds.append(temp2)\n",
        "#   temp3 = np.zeros((embedding_dim), dtype=np.float32)\n",
        "#   while len(sent_embeds) < max_length:\n",
        "#     sent_embeds.append(temp3)\n",
        "#   input_seq.append(sent_embeds)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQ4FwIjsSZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_labs = []\n",
        "\n",
        "# for i in range(500):\n",
        "#   sent_embeds = []\n",
        "#   temp1 = dataset[2*i+1].split()\n",
        "#   for j in temp1:\n",
        "#     if j in embeds:\n",
        "#       temp2 = embeds[j]\n",
        "#     else:\n",
        "#       temp2 = np.zeros((embedding_dim), dtype=np.float32)\n",
        "#     sent_embeds.append(temp2)\n",
        "#   temp3 = np.zeros((embedding_dim), dtype=np.float32)\n",
        "#   while len(sent_embeds) < max_length:\n",
        "#     sent_embeds.append(temp3)\n",
        "#   input_labs.append(sent_embeds)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3AFhb8JZK4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i = 1\n",
        "\n",
        "# print(input_labs[i])\n",
        "# print(dataset[2*i+1].split())\n",
        "\n",
        "# print(embeds['promised'])\n",
        "\n",
        "# for l in dataset[2*i].split():\n",
        "#   a = l not in embeds\n",
        "#   print(a)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nbuZxAAh1GE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final_input_seq = np.array(input_seq)\n",
        "# final_input_labs = np.array(input_labs)\n",
        "# print(final_input_seq.shape)\n",
        "# print(final_input_seq[1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h8koCgd-4vC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0acb8214-7b56-4d85-e1e7-ac3607b77176"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMH0Up8ejGYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_word_embeddings(file_path):\n",
        "    \n",
        "    with open(file_path , 'r') as f:\n",
        "        wordToEmbedding = {}\n",
        "        wordToIndex = {}\n",
        "        indexToWord = {}\n",
        "        \n",
        "        for line in f:\n",
        "            data = line.strip().split()\n",
        "            token = data[0]\n",
        "            wordToEmbedding[token] = np.array(data[1:] ,dtype = np.float64)\n",
        "\n",
        "        tokens = sorted(wordToEmbedding.keys())\n",
        "        for idx , token in enumerate(tokens):\n",
        "            idx = idx + 1 #for zero masking\n",
        "            wordToIndex[token] = idx\n",
        "            indexToWord[idx] = token\n",
        "\n",
        "    return wordToEmbedding , wordToIndex , indexToWord"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1woLwIxjjasN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_pretrained_embedding_layer(wordToEmbedding , wordToIndex , indexToWord):\n",
        "    \n",
        "    vocablen = 20000 #for zero masking\n",
        "    embedding_dimensions = 200\n",
        "    \n",
        "    embeddingMatrix = np.zeros((vocablen , embedding_dimensions))\n",
        "    for word , index in word_index.items():\n",
        "        if word not in wordToEmbedding.keys():\n",
        "            embeddingMatrix[index ,:] = np.random.uniform(low = 0 , high =1 ,size = (1,200))\n",
        "            if index == vocablen-1:\n",
        "              break\n",
        "        else :\n",
        "            embeddingMatrix[index , :] = wordToEmbedding[word]\n",
        "            if index == vocablen-1:\n",
        "              break\n",
        "        \n",
        "    embeddingLayer = Embedding(vocablen , embedding_dimensions , weights = [embeddingMatrix] , trainable = False)\n",
        "    print(embeddingMatrix.shape)\n",
        "    \n",
        "    return embeddingLayer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQGzhDV3jHQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordToEmbedding , wordToIndex , indexToWord = create_word_embeddings('glove.6B.200d.txt')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJfPXMmCjcF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6842338-81c2-4c2f-fccc-01e2f146c452"
      },
      "source": [
        "embeddingLayer = create_pretrained_embedding_layer(wordToEmbedding , wordToIndex , indexToWord)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN2haV4xkawv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "230f25fa-ee56-4dec-d2d0-bc9ab46a2e5d"
      },
      "source": [
        "print(len(word_index))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Z8lWf0C2lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4efZlsy4BnuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(max_length)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ6B6R22CoAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = densor2(e)\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(energies)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTdmEtPTDM7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 128 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 128 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# Please note, this is the post attention LSTM cell.  \n",
        "# For the purposes of passing the automatic grader\n",
        "# please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
        "output_layer = Dense(vocab_size, activation=softmax)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yUbqv8ZDrDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X = Input(shape=(Tx,))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    e = embeddingLayer(X)\n",
        "    a = Bidirectional(LSTM(units = n_a ,return_sequences = True))(e)\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(inputs = context,initial_state = [s,c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs = [X,s0,c0],outputs = outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNpAZ4q3D78S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model(20, 20, n_a, n_s, 20000, 20000)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0XkY7EqFE0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1d97588-0a40-4f0c-c7e2-7fc79d1788ba"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 200)      4000000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 20, 256)      336896      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 20, 128)      0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "                                                                 lstm_1[10][0]                    \n",
            "                                                                 lstm_1[11][0]                    \n",
            "                                                                 lstm_1[12][0]                    \n",
            "                                                                 lstm_1[13][0]                    \n",
            "                                                                 lstm_1[14][0]                    \n",
            "                                                                 lstm_1[15][0]                    \n",
            "                                                                 lstm_1[16][0]                    \n",
            "                                                                 lstm_1[17][0]                    \n",
            "                                                                 lstm_1[18][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 20, 384)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[10][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[11][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[12][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[13][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[14][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[15][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[16][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[17][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[18][0]           \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[19][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 20, 10)       3850        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "                                                                 concatenate_1[10][0]             \n",
            "                                                                 concatenate_1[11][0]             \n",
            "                                                                 concatenate_1[12][0]             \n",
            "                                                                 concatenate_1[13][0]             \n",
            "                                                                 concatenate_1[14][0]             \n",
            "                                                                 concatenate_1[15][0]             \n",
            "                                                                 concatenate_1[16][0]             \n",
            "                                                                 concatenate_1[17][0]             \n",
            "                                                                 concatenate_1[18][0]             \n",
            "                                                                 concatenate_1[19][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 20, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "                                                                 dense_1[10][0]                   \n",
            "                                                                 dense_1[11][0]                   \n",
            "                                                                 dense_1[12][0]                   \n",
            "                                                                 dense_1[13][0]                   \n",
            "                                                                 dense_1[14][0]                   \n",
            "                                                                 dense_1[15][0]                   \n",
            "                                                                 dense_1[16][0]                   \n",
            "                                                                 dense_1[17][0]                   \n",
            "                                                                 dense_1[18][0]                   \n",
            "                                                                 dense_1[19][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 20, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "                                                                 dense_2[10][0]                   \n",
            "                                                                 dense_2[11][0]                   \n",
            "                                                                 dense_2[12][0]                   \n",
            "                                                                 dense_2[13][0]                   \n",
            "                                                                 dense_2[14][0]                   \n",
            "                                                                 dense_2[15][0]                   \n",
            "                                                                 dense_2[16][0]                   \n",
            "                                                                 dense_2[17][0]                   \n",
            "                                                                 dense_2[18][0]                   \n",
            "                                                                 dense_2[19][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 256)       0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[10][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[11][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[12][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[13][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[14][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[15][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[16][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[17][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[18][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[19][0]         \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 128), (None, 197120      dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "                                                                 dot_1[10][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "                                                                 lstm_1[9][2]                     \n",
            "                                                                 dot_1[11][0]                     \n",
            "                                                                 lstm_1[10][0]                    \n",
            "                                                                 lstm_1[10][2]                    \n",
            "                                                                 dot_1[12][0]                     \n",
            "                                                                 lstm_1[11][0]                    \n",
            "                                                                 lstm_1[11][2]                    \n",
            "                                                                 dot_1[13][0]                     \n",
            "                                                                 lstm_1[12][0]                    \n",
            "                                                                 lstm_1[12][2]                    \n",
            "                                                                 dot_1[14][0]                     \n",
            "                                                                 lstm_1[13][0]                    \n",
            "                                                                 lstm_1[13][2]                    \n",
            "                                                                 dot_1[15][0]                     \n",
            "                                                                 lstm_1[14][0]                    \n",
            "                                                                 lstm_1[14][2]                    \n",
            "                                                                 dot_1[16][0]                     \n",
            "                                                                 lstm_1[15][0]                    \n",
            "                                                                 lstm_1[15][2]                    \n",
            "                                                                 dot_1[17][0]                     \n",
            "                                                                 lstm_1[16][0]                    \n",
            "                                                                 lstm_1[16][2]                    \n",
            "                                                                 dot_1[18][0]                     \n",
            "                                                                 lstm_1[17][0]                    \n",
            "                                                                 lstm_1[17][2]                    \n",
            "                                                                 dot_1[19][0]                     \n",
            "                                                                 lstm_1[18][0]                    \n",
            "                                                                 lstm_1[18][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 20000)        2580000     lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "                                                                 lstm_1[10][0]                    \n",
            "                                                                 lstm_1[11][0]                    \n",
            "                                                                 lstm_1[12][0]                    \n",
            "                                                                 lstm_1[13][0]                    \n",
            "                                                                 lstm_1[14][0]                    \n",
            "                                                                 lstm_1[15][0]                    \n",
            "                                                                 lstm_1[16][0]                    \n",
            "                                                                 lstm_1[17][0]                    \n",
            "                                                                 lstm_1[18][0]                    \n",
            "                                                                 lstm_1[19][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,117,877\n",
            "Trainable params: 3,117,877\n",
            "Non-trainable params: 4,000,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce356qa4FIIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODE HERE ### (≈2 lines)\n",
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(opt,metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R72dTHpgFcUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0 = np.zeros((10000, n_s))\n",
        "c0 = np.zeros((10000, n_s))\n",
        "outputs = list(input_labs.swapaxes(0,1))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5h3S47NGGen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "497145d2-7aeb-495f-96dd-7f83eaf9a5d7"
      },
      "source": [
        "model.fit([input_seq, s0, c0], outputs, epochs=5, batch_size=100)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "10000/10000 [==============================] - 159s 16ms/step - loss: 55.7823 - dense_3_loss: 1.0105 - dense_3_accuracy: 0.0039 - dense_3_accuracy_1: 0.1475 - dense_3_accuracy_2: 0.3206 - dense_3_accuracy_3: 0.4403 - dense_3_accuracy_4: 0.5501 - dense_3_accuracy_5: 0.6502 - dense_3_accuracy_6: 0.7288 - dense_3_accuracy_7: 0.7872 - dense_3_accuracy_8: 0.8321 - dense_3_accuracy_9: 0.8710 - dense_3_accuracy_10: 0.8998 - dense_3_accuracy_11: 0.9205 - dense_3_accuracy_12: 0.9367 - dense_3_accuracy_13: 0.9477 - dense_3_accuracy_14: 0.9550 - dense_3_accuracy_15: 0.9619 - dense_3_accuracy_16: 0.9675 - dense_3_accuracy_17: 0.9715 - dense_3_accuracy_18: 0.9749 - dense_3_accuracy_19: 0.9769\n",
            "Epoch 2/5\n",
            "10000/10000 [==============================] - 147s 15ms/step - loss: 42.6170 - dense_3_loss: 0.3521 - dense_3_accuracy: 0.0039 - dense_3_accuracy_1: 0.1492 - dense_3_accuracy_2: 0.3238 - dense_3_accuracy_3: 0.4456 - dense_3_accuracy_4: 0.5556 - dense_3_accuracy_5: 0.6571 - dense_3_accuracy_6: 0.7363 - dense_3_accuracy_7: 0.7949 - dense_3_accuracy_8: 0.8402 - dense_3_accuracy_9: 0.8795 - dense_3_accuracy_10: 0.9089 - dense_3_accuracy_11: 0.9299 - dense_3_accuracy_12: 0.9461 - dense_3_accuracy_13: 0.9571 - dense_3_accuracy_14: 0.9644 - dense_3_accuracy_15: 0.9713 - dense_3_accuracy_16: 0.9770 - dense_3_accuracy_17: 0.9810 - dense_3_accuracy_18: 0.9846 - dense_3_accuracy_19: 0.9867\n",
            "Epoch 3/5\n",
            "10000/10000 [==============================] - 147s 15ms/step - loss: 42.0446 - dense_3_loss: 0.3391 - dense_3_accuracy: 0.0039 - dense_3_accuracy_1: 0.1492 - dense_3_accuracy_2: 0.3238 - dense_3_accuracy_3: 0.4456 - dense_3_accuracy_4: 0.5556 - dense_3_accuracy_5: 0.6571 - dense_3_accuracy_6: 0.7363 - dense_3_accuracy_7: 0.7949 - dense_3_accuracy_8: 0.8402 - dense_3_accuracy_9: 0.8795 - dense_3_accuracy_10: 0.9089 - dense_3_accuracy_11: 0.9299 - dense_3_accuracy_12: 0.9461 - dense_3_accuracy_13: 0.9571 - dense_3_accuracy_14: 0.9644 - dense_3_accuracy_15: 0.9713 - dense_3_accuracy_16: 0.9770 - dense_3_accuracy_17: 0.9810 - dense_3_accuracy_18: 0.9846 - dense_3_accuracy_19: 0.9867\n",
            "Epoch 4/5\n",
            "10000/10000 [==============================] - 147s 15ms/step - loss: 41.3520 - dense_3_loss: 0.3222 - dense_3_accuracy: 0.0039 - dense_3_accuracy_1: 0.1492 - dense_3_accuracy_2: 0.3238 - dense_3_accuracy_3: 0.4456 - dense_3_accuracy_4: 0.5556 - dense_3_accuracy_5: 0.6571 - dense_3_accuracy_6: 0.7363 - dense_3_accuracy_7: 0.7949 - dense_3_accuracy_8: 0.8402 - dense_3_accuracy_9: 0.8795 - dense_3_accuracy_10: 0.9089 - dense_3_accuracy_11: 0.9299 - dense_3_accuracy_12: 0.9461 - dense_3_accuracy_13: 0.9571 - dense_3_accuracy_14: 0.9644 - dense_3_accuracy_15: 0.9713 - dense_3_accuracy_16: 0.9770 - dense_3_accuracy_17: 0.9810 - dense_3_accuracy_18: 0.9846 - dense_3_accuracy_19: 0.9867\n",
            "Epoch 5/5\n",
            "10000/10000 [==============================] - 147s 15ms/step - loss: 40.3768 - dense_3_loss: 0.2842 - dense_3_accuracy: 0.0039 - dense_3_accuracy_1: 0.1492 - dense_3_accuracy_2: 0.3238 - dense_3_accuracy_3: 0.4456 - dense_3_accuracy_4: 0.5556 - dense_3_accuracy_5: 0.6571 - dense_3_accuracy_6: 0.7363 - dense_3_accuracy_7: 0.7949 - dense_3_accuracy_8: 0.8402 - dense_3_accuracy_9: 0.8795 - dense_3_accuracy_10: 0.9089 - dense_3_accuracy_11: 0.9299 - dense_3_accuracy_12: 0.9461 - dense_3_accuracy_13: 0.9571 - dense_3_accuracy_14: 0.9644 - dense_3_accuracy_15: 0.9713 - dense_3_accuracy_16: 0.9770 - dense_3_accuracy_17: 0.9810 - dense_3_accuracy_18: 0.9846 - dense_3_accuracy_19: 0.9867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f857ebc4518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DItkSNZOGTMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def beam_search_decoder(predictions, top_k = 5):\n",
        "\n",
        "    output_sequences = [([], 0)]\n",
        "    \n",
        "    for token_probs in predictions:\n",
        "        new_sequences = []\n",
        "        token_probs = token_probs.reshape(20000 ,)\n",
        "        for old_seq, old_score in output_sequences:\n",
        "            for char_index in range(len(token_probs)):\n",
        "                new_seq = old_seq + [char_index]\n",
        "                new_score = old_score + math.log(token_probs[char_index])\n",
        "                new_sequences.append((new_seq, new_score))\n",
        "                \n",
        "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
        "        output_sequences = output_sequences[:top_k]\n",
        "        \n",
        "    return output_sequences"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORwuwhI_HUZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e97cf9b-1334-4993-dde6-c983c4405a9f"
      },
      "source": [
        "s = [(\"what is your name\")]\n",
        "seq = tokenizer.texts_to_sequences(s)\n",
        "pad = pad_sequences(seq,maxlen=max_length, truncating=trunc_type,padding = 'post')\n",
        "print(pad)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 15  13  18 215   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGA4UfSFt0W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict([pad , np.zeros((1, n_s)) , np.zeros((1, n_s))])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2JRsCxPuAns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef4316f7-97b5-48a5-fbba-67eea32a927f"
      },
      "source": [
        "pred[1].shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrIUzApjuP1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c71d0aaa-2a0b-4ef5-c8eb-7683de2d10ec"
      },
      "source": [
        "len(pred)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGpNmlgpufpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e6346295-4707-4169-baf2-1542a1e70580"
      },
      "source": [
        "ans = beam_search_decoder(pred)\n",
        "ans"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  -6.955544819619267),\n",
              " ([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  -9.427976481762903),\n",
              " ([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  -9.637861905639388),\n",
              " ([4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  -9.793647604894536),\n",
              " ([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  -9.85820191156175)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsCHDT4pulc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6146938d-d44f-430c-f456-0b1bb967867a"
      },
      "source": [
        "print(index_word[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Q_tMH7vYZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}